"""
VeriFuse Hunter Engine — Colorado Surplus Dragnet
==================================================
Targets: 17 Colorado counties across 4 platform types.
Output:  Litigation-quality asset dicts compatible with verifuse.core.pipeline.ingest_asset().

PLATFORM MAP (verified 2026-02-07):
  RealForeclose (6): Eagle, El Paso, Larimer, Mesa, Summit, Weld
  GTS Search    (5): Adams, Arapahoe, Boulder, Douglas, Garfield
  County Page   (4): Pitkin, Routt, San Miguel, Grand
  Standard      (2): Denver, Jefferson

MODULES:
  A. StealthSession        — UA rotation, cookie persistence, retry logic
  B. ForensicScraper       — Generic HTML table/div parser (Denver, Jefferson)
  B2. RealForecloseScraper — Calendar + rowA/rowB auction table parser
  B3. GTSSearchScraper     — ASP.NET form search + pagination
  B4. CountyPageScraper    — .gov page + PDF Foreclosure Book support
  C. OCRPatcher            — PDF bid-sheet extraction via pdfplumber
  D. LienWiper             — Surplus math, whale classification, lien analysis

USAGE:
  from verifuse.scrapers.hunter_engine import run_hunter
  results = run_hunter(start_year=2020, end_year=2026)

ETHICAL CONSTRAINTS:
  - Targets PUBLIC RECORD data only (no login walls, no CAPTCHA bypass)
  - Respectful rate limiting (2-5s random delay between requests)
  - 30s backoff on failure, max 3 retries per request
  - Identifies as a research tool via UA (still rotated for session stability)
"""

import hashlib
import io
import json
import logging
import os
import random
import re
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import quote_plus, urljoin

import requests
from bs4 import BeautifulSoup

# Optional imports — graceful degradation if not installed
try:
    # from fake_useragent import UserAgent
    _UA = UserAgent(browsers=["chrome", "firefox", "edge"])
except ImportError:
    _UA = None

try:
    import pdfplumber
except ImportError:
    pdfplumber = None


# ============================================================================
# LOGGING
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
log = logging.getLogger("verifuse.hunter")


# ============================================================================
# MODULE A: STEALTH SESSION
# ============================================================================

class StealthSession:
    """
    HTTP session wrapper with:
      - Rotating User-Agent headers (via fake_useragent)
      - Persistent cookies (handles ASP.NET ViewState/SessionID)
      - Randomized delays between requests (2-5 seconds)
      - Automatic retry with 30-second backoff on failure
      - Request counting for rate monitoring
    """

    # Fallback UAs if fake_useragent isn't installed or fails
    FALLBACK_UAS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    ]

    MAX_RETRIES = 3
    RETRY_BACKOFF_SECONDS = 30
    MIN_DELAY = 2.0
    MAX_DELAY = 5.0

    def __init__(self):
        self.session = requests.Session()
        self.request_count = 0
        self.error_count = 0
        self._rotate_ua()

        # Standard headers that mimic a real browser
        self.session.headers.update({
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Referer": "https://www.google.com/",
        })

    def _rotate_ua(self):
        """Pick a new User-Agent string."""
        if _UA:
            try:
                ua = _UA.random
            except Exception:
                ua = random.choice(self.FALLBACK_UAS)
        else:
            ua = random.choice(self.FALLBACK_UAS)
        self.session.headers["User-Agent"] = ua

    def _sleep(self):
        """Random delay between requests to avoid detection."""
        delay = random.uniform(self.MIN_DELAY, self.MAX_DELAY)
        log.debug(f"Sleeping {delay:.1f}s between requests")
        time.sleep(delay)

    def get(self, url: str, **kwargs) -> Optional[requests.Response]:
        """
        GET with retry logic.
        Returns Response on success, None after all retries exhausted.
        """
        return self._request("GET", url, **kwargs)

    def post(self, url: str, **kwargs) -> Optional[requests.Response]:
        """POST with retry logic."""
        return self._request("POST", url, **kwargs)

    def _request(self, method: str, url: str, **kwargs) -> Optional[requests.Response]:
        """
        Core request method with:
          1. Pre-request delay (rate limiting)
          2. UA rotation every 10 requests
          3. Up to MAX_RETRIES attempts with RETRY_BACKOFF_SECONDS wait on failure
        """
        # Rotate UA periodically to look like different sessions
        if self.request_count > 0 and self.request_count % 10 == 0:
            self._rotate_ua()

        # Rate-limiting delay (skip on first request of session)
        if self.request_count > 0:
            self._sleep()

        kwargs.setdefault("timeout", 30)

        for attempt in range(1, self.MAX_RETRIES + 1):
            try:
                self.request_count += 1
                log.debug(f"[{method}] {url} (attempt {attempt}/{self.MAX_RETRIES})")
                resp = self.session.request(method, url, **kwargs)
                resp.raise_for_status()
                return resp

            except requests.exceptions.HTTPError as e:
                status = e.response.status_code if e.response else "?"
                log.warning(f"HTTP {status} on {url} (attempt {attempt})")

                # 403/429 = we're being blocked. Back off hard.
                if status in (403, 429):
                    wait = self.RETRY_BACKOFF_SECONDS * attempt
                    log.warning(f"Rate limited or blocked. Waiting {wait}s...")
                    time.sleep(wait)
                    self._rotate_ua()  # Switch identity
                elif attempt < self.MAX_RETRIES:
                    time.sleep(self.RETRY_BACKOFF_SECONDS)

            except requests.exceptions.ConnectionError:
                log.warning(f"Connection error on {url} (attempt {attempt})")
                if attempt < self.MAX_RETRIES:
                    time.sleep(self.RETRY_BACKOFF_SECONDS)

            except requests.exceptions.Timeout:
                log.warning(f"Timeout on {url} (attempt {attempt})")
                if attempt < self.MAX_RETRIES:
                    time.sleep(self.RETRY_BACKOFF_SECONDS)

            except Exception as e:
                log.error(f"Unexpected error on {url}: {e}")
                self.error_count += 1
                if attempt < self.MAX_RETRIES:
                    time.sleep(self.RETRY_BACKOFF_SECONDS)

        log.error(f"FAILED after {self.MAX_RETRIES} attempts: {url}")
        self.error_count += 1
        return None

    def download_pdf(self, url: str) -> Optional[bytes]:
        """Download a PDF file. Returns raw bytes or None."""
        resp = self.get(url)
        if resp and resp.content:
            # Basic validation — PDFs start with %PDF
            if resp.content[:5] == b"%PDF-":
                return resp.content
            else:
                log.warning(f"Downloaded content from {url} is not a valid PDF")
        return None

    def stats(self) -> dict:
        return {
            "requests": self.request_count,
            "errors": self.error_count,
            "current_ua": self.session.headers.get("User-Agent", "")[:60] + "...",
        }


# ============================================================================
# MODULE B: FORENSIC SCRAPER — Denver Public Trustee
# ============================================================================

# ---------------------------------------------------------------------------
# SELECTOR CONFIGS — Update these when county sites change their HTML layout.
# Keeping selectors in dicts means you fix ONE line instead of hunting through
# parser code when a site redesigns.
# ---------------------------------------------------------------------------

DENVER_CONFIG = {
    "base_url": "https://www.denvergov.org/Government/Agencies-Departments-Offices/"
                "Agencies-Departments-Offices-Directory/Denver-Clerk-and-Recorder/"
                "Recording-Division/Denver-Public-Trustee",
    # Denver Public Trustee — updated Feb 2026 after site restructure
    "search_url": "https://www.denvergov.org/Government/Agencies-Departments-Offices/"
                  "Agencies-Departments-Offices-Directory/Denver-Clerk-and-Recorder/"
                  "Recording-Division/Denver-Public-Trustee",
    "recorder_url_template": "https://denvergov.org/recorder/search?query={owner}",
    "county": "Denver",
    "state": "CO",
    "asset_type": "FORECLOSURE_SURPLUS",
    "source_name": "denver_foreclosure",  # Must match scraper_registry
}

JEFFERSON_CONFIG = {
    "base_url": "https://www.jeffco.us/807/Public-Trustee",
    "search_url": "https://www.jeffco.us/807/Public-Trustee",
    "recorder_url_template": (
        "https://gts.co.jefferson.co.us/recorder/eagleweb/"
        "docSearch.jsp?search={owner}"
    ),
    "county": "Jefferson",
    "state": "CO",
    "asset_type": "FORECLOSURE_SURPLUS",
    "source_name": "jefferson_foreclosure",  # Must match scraper_registry
}


# ============================================================================
# REALFORECLOSE COUNTIES — Direct Vendor Endpoints (VERIFIED LIVE)
# ============================================================================
# RealForeclose.com: Active auction platform for 6 Colorado counties.
# URL pattern: https://{county}.realforeclose.com/index.cfm?zaction=...
# Calendar → Auction Preview → Auction Details (rowA/rowB table structure)
#
# VERIFIED 2026-02-07: All 6 subdomains return 200 with 22-24KB calendar pages.
# Dead subdomains (pitkin, routt, adams, arapahoe, douglas, boulder, garfield,
# grand) redirect to realauction.com corporate homepage — NOT auction data.

REALFORECLOSE_COUNTIES = {
    "Eagle": {
        "base_url": "https://eagle.realforeclose.com",
        "search_url": "https://eagle.realforeclose.com/index.cfm?zaction=USER&zmethod=CALENDAR",
        "auction_url": "https://eagle.realforeclose.com/index.cfm?zaction=AUCTION&zmethod=PREVIEW",
        "recorder_url_template": "https://eaglecounty.us/Clerk/Recording/Search?name={owner}",
        "county": "Eagle",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "eagle_portal",
        "platform": "realforeclose",
    },
    "El Paso": {
        "base_url": "https://elpasoco.realforeclose.com",
        "search_url": "https://elpasoco.realforeclose.com/index.cfm?zaction=USER&zmethod=CALENDAR",
        "auction_url": "https://elpasoco.realforeclose.com/index.cfm?zaction=AUCTION&zmethod=PREVIEW",
        "recorder_url_template": "https://www.elpasoco.com/recorder/search?query={owner}",
        "county": "El Paso",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "elpaso_foreclosure",
        "platform": "realforeclose",
    },
    "Larimer": {
        "base_url": "https://larimer.realforeclose.com",
        "search_url": "https://larimer.realforeclose.com/index.cfm?zaction=USER&zmethod=CALENDAR",
        "auction_url": "https://larimer.realforeclose.com/index.cfm?zaction=AUCTION&zmethod=PREVIEW",
        "recorder_url_template": "https://www.larimer.gov/clerk/recording/search?name={owner}",
        "county": "Larimer",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "larimer_foreclosure",
        "platform": "realforeclose",
    },
    "Mesa": {
        "base_url": "https://mesa.realforeclose.com",
        "search_url": "https://mesa.realforeclose.com/index.cfm?zaction=USER&zmethod=CALENDAR",
        "auction_url": "https://mesa.realforeclose.com/index.cfm?zaction=AUCTION&zmethod=PREVIEW",
        "recorder_url_template": "https://www.mesacounty.us/clerk-and-recorder/recording/search?name={owner}",
        "county": "Mesa",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "mesa_foreclosure",
        "platform": "realforeclose",
    },
    "Summit": {
        "base_url": "https://summit.realforeclose.com",
        "search_url": "https://summit.realforeclose.com/index.cfm?zaction=USER&zmethod=CALENDAR",
        "auction_url": "https://summit.realforeclose.com/index.cfm?zaction=AUCTION&zmethod=PREVIEW",
        "recorder_url_template": "https://www.summitcountyco.gov/recorder/search?query={owner}",
        "county": "Summit",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "summit_govease",
        "platform": "realforeclose",
    },
    "Weld": {
        "base_url": "https://weld.realforeclose.com",
        "search_url": "https://weld.realforeclose.com/index.cfm?zaction=USER&zmethod=CALENDAR",
        "auction_url": "https://weld.realforeclose.com/index.cfm?zaction=AUCTION&zmethod=PREVIEW",
        "recorder_url_template": "https://www.weld.gov/recorder/search?query={owner}",
        "county": "Weld",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "weld_foreclosure",
        "platform": "realforeclose",
    },
}


# ============================================================================
# GTS SEARCH COUNTIES — County-Hosted Foreclosure Search Databases
# ============================================================================
# These counties run their own ASP.NET GTS (Government Technology Solutions)
# or custom web apps for foreclosure search. Verified LIVE 2026-02-07.
# Data format: HTML tables with search forms, sometimes downloadable reports.

GTS_COUNTIES = {
    "Adams": {
        "base_url": "https://apps.adcogov.org/PTForeclosureSearch/",
        "search_url": "https://apps.adcogov.org/PTForeclosureSearch/",
        "reports_url": "https://apps.adcogov.org/PTForeclosureSearch/reports",
        "recorder_url_template": "http://recording.adcogov.org/LandmarkWeb/search/index?nameFilter={owner}",
        "county": "Adams",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "adams_foreclosure",
        "platform": "gts",
    },
    "Arapahoe": {
        "base_url": "https://foreclosuresearch.arapahoegov.com/",
        "search_url": "https://foreclosuresearch.arapahoegov.com/foreclosure/",
        "recorder_url_template": "https://clerk.arapahoeco.gov/recorder/eagleweb/docSearch.jsp?search={owner}",
        "county": "Arapahoe",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "arapahoe_foreclosure",
        "platform": "gts",
    },
    "Boulder": {
        "base_url": "http://www.bouldercountypt.org/GTSSearch/",
        "search_url": "http://www.bouldercountypt.org/GTSSearch/index.aspx?ds=1",
        "recorder_url_template": "https://recorder.bouldercounty.gov/search?name={owner}",
        "county": "Boulder",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "boulder_foreclosure",
        "platform": "gts",
    },
    "Douglas": {
        "base_url": "https://apps.douglas.co.us/gts/",
        "search_url": "https://apps.douglas.co.us/gts/",
        "recorder_url_template": "https://apps.douglas.co.us/recorder/web/search?name={owner}",
        "county": "Douglas",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "douglas_foreclosure",
        "platform": "gts",
    },
    "Garfield": {
        "base_url": "https://foreclosures.garfield-county.com/",
        "search_url": "https://foreclosures.garfield-county.com/index.aspx",
        "govease_url": "https://liveauctions.govease.com/CO/cogarfieldforeclosure/1324/browsestandard",
        "recorder_url_template": "https://www.garfield-county.com/clerk/recording/search?name={owner}",
        "county": "Garfield",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "garfield_foreclosure",
        "platform": "gts",
    },
}


# ============================================================================
# COUNTY PAGE COUNTIES — Direct .gov Scraping
# ============================================================================
# These counties don't use vendor auction platforms. Data lives on their
# official .gov websites as HTML pages, sometimes with linked PDFs.

COUNTY_PAGE_COUNTIES = {
    "Pitkin": {
        "base_url": "https://pitkincounty.com/294/Foreclosures",
        "search_url": "https://pitkincounty.com/325/Foreclosure-Search",
        "recorder_url_template": "https://www.pitkinclerk.com/clerk-recorder/search?q={owner}",
        "county": "Pitkin",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "pitkin_foreclosure",
        "platform": "county_page",
    },
    "Routt": {
        "base_url": "https://www.co.routt.co.us/414/Public-Trustee",
        "search_url": "https://www.co.routt.co.us/679/Foreclosure-Sale",
        "recorder_url_template": "https://www.co.routt.co.us/recorder/search?query={owner}",
        "county": "Routt",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "routt_foreclosure",
        "platform": "county_page",
    },
    "San Miguel": {
        "base_url": "https://www.sanmiguelcountyco.gov/199/Public-Trustee",
        "search_url": "https://www.sanmiguelcountyco.gov/199/Public-Trustee",
        "recorder_url_template": "https://www.sanmiguelcountyco.gov/recorder/search?query={owner}",
        "county": "San Miguel",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "sanmiguel_portal",
        "platform": "county_page",
    },
    "Grand": {
        "base_url": "https://www.co.grand.co.us/137/Public-Trustee",
        "search_url": "https://www.co.grand.co.us/137/Public-Trustee",
        "pdf_url": "https://www.co.grand.co.us/DocumentCenter/View/27356/Foreclosure-Book-2025",
        "recorder_url_template": "https://www.co.grand.co.us/clerk/recording/search?name={owner}",
        "county": "Grand",
        "state": "CO",
        "asset_type": "FORECLOSURE_SURPLUS",
        "source_name": "grand_foreclosure",
        "platform": "county_page",
    },
}


def clean_money(raw: Any) -> Optional[float]:
    """
    Extract a numeric dollar amount from messy text.

    Handles:
      "$1,234.56"  → 1234.56
      "(1,234.56)" → -1234.56   (accounting negatives)
      "1234"       → 1234.0
      "$0.00"      → 0.0
      "N/A"        → None
    """
    if raw is None:
        return None
    text = str(raw).strip()
    if not text or text.upper() in ("N/A", "NA", "-", "", "NONE", "TBD"):
        return None

    # Accounting-style negatives: (1,234.56) → -1234.56
    negative = "(" in text and ")" in text
    text = re.sub(r"[^0-9.]", "", text)

    try:
        val = float(text)
        return -val if negative else val
    except (ValueError, TypeError):
        return None


def clean_owner(name: Any) -> Optional[str]:
    """
    Normalize owner name.
    Strips legal suffixes, normalizes whitespace, title-cases.
    """
    if not name:
        return None
    text = str(name).strip()
    if text.upper() in ("UNKNOWN", "N/A", "", "NONE"):
        return None

    # Remove common legal suffixes that don't help identification
    for suffix in (" ET AL", " ET UX", " ETAL", " ET VIR"):
        text = text.replace(suffix, "").replace(suffix.lower(), "")

    # Collapse whitespace
    text = re.sub(r"\s+", " ", text).strip()

    # Title case unless it looks like an entity (has LLC, INC, etc.)
    if not re.search(r"\b(LLC|INC|CORP|LP|LLP|TRUST|BANK|HOA)\b", text.upper()):
        text = text.title()

    return text if text else None


def normalize_address(addr: Any) -> Optional[str]:
    """Standardize street abbreviations for matching."""
    if not addr:
        return None
    text = str(addr).strip().upper()
    if text in ("UNKNOWN", "N/A", "", "NONE"):
        return None

    replacements = {
        "STREET": "ST", "AVENUE": "AVE", "BOULEVARD": "BLVD",
        "DRIVE": "DR", "COURT": "CT", "PLACE": "PL",
        "LANE": "LN", "ROAD": "RD", "CIRCLE": "CIR",
        "PARKWAY": "PKWY", "TERRACE": "TER", "TRAIL": "TRL",
        "HIGHWAY": "HWY", "NORTH": "N", "SOUTH": "S",
        "EAST": "E", "WEST": "W",
    }
    for full, abbr in replacements.items():
        text = re.sub(rf"\b{full}\b", abbr, text)

    return re.sub(r"\s+", " ", text).strip()


def detect_absentee(property_addr: Optional[str],
                    mailing_addr: Optional[str]) -> Tuple[bool, str]:
    """
    THE ZOMBIE ALGORITHM
    ====================
    Compare property address vs mailing address.
    If they differ → owner has already moved → "ABSENTEE OWNER"
    This means less friction for attorney contact (owner isn't occupying).

    Returns: (is_absentee: bool, reason: str)
    """
    if not property_addr or not mailing_addr:
        return False, "insufficient_data"

    prop = normalize_address(property_addr)
    mail = normalize_address(mailing_addr)

    if not prop or not mail:
        return False, "insufficient_data"

    # Exact match after normalization → owner lives at property
    if prop == mail:
        return False, "owner_occupant"

    # Check if one contains the other (partial match — same building, different unit)
    if prop in mail or mail in prop:
        return False, "partial_match_same_location"

    # Different addresses → absentee owner confirmed
    return True, "address_mismatch"


def parse_date(raw: Any) -> Optional[str]:
    """
    Parse various date formats into ISO 8601 (YYYY-MM-DD).
    Handles: MM/DD/YYYY, MM-DD-YYYY, YYYY-MM-DD, Mon DD YYYY, etc.
    """
    if not raw:
        return None
    text = str(raw).strip()
    if text.upper() in ("N/A", "NONE", "", "TBD"):
        return None

    formats = [
        "%m/%d/%Y", "%m-%d-%Y", "%Y-%m-%d", "%Y/%m/%d",
        "%B %d, %Y", "%b %d, %Y", "%m/%d/%y", "%m-%d-%y",
        "%d-%b-%Y", "%d %B %Y",
    ]
    for fmt in formats:
        try:
            return datetime.strptime(text, fmt).strftime("%Y-%m-%d")
        except ValueError:
            continue
    return None


def generate_asset_hash(county: str, case_number: str,
                        property_address: str) -> str:
    """Deterministic hash for deduplication — matches pipeline.py logic."""
    parts = sorted([
        str(county).lower().strip(),
        str(case_number).lower().strip(),
        str(property_address).lower().strip(),
    ])
    return hashlib.sha256("|".join(parts).encode()).hexdigest()[:16]


class ForensicScraper:
    """
    MODULE B: Denver & Jefferson County Public Trustee scraper.

    Strategy:
      1. Hit the county's public trustee surplus/foreclosure page
      2. Parse the HTML table(s) of completed sales
      3. Extract: bid, debt, surplus, owner, property address, mailing address
      4. Run the Zombie Algorithm (absentee owner detection)
      5. Output: list of dicts compatible with pipeline.ingest_asset()

    IMPORTANT — HTML Structure Reality:
      County sites change layouts frequently. This scraper uses a
      CONFIGURABLE selector approach. If the site changes, you update
      the config dict — not the parser logic.
    """

    def __init__(self, session: StealthSession, config: dict):
        self.session = session
        self.config = config
        self.results: List[Dict] = []
        self.errors: List[str] = []

    def scrape(self, start_year: int = 2020,
               end_year: int = 2026) -> List[Dict]:
        """
        Main entry point. Scrapes surplus records for a date range.

        Since Denver/Jefferson don't always have a date-range API endpoint,
        this method:
          1. Fetches the main foreclosure sales page
          2. Looks for links to historical sale data (year-by-year or paginated)
          3. Parses each page for tabular surplus data
          4. Falls back to parsing whatever table data is on the main page

        Returns: List of asset dicts ready for pipeline ingestion.
        """
        log.info(f"[{self.config['county']}] Starting scrape: {start_year}-{end_year}")

        # Step 1: Fetch the main page
        resp = self.session.get(self.config["search_url"])
        if not resp:
            self.errors.append(f"Failed to fetch main page: {self.config['search_url']}")
            return self.results

        soup = BeautifulSoup(resp.text, "lxml")

        # Step 2: Look for links to surplus/overbid/excess data
        data_links = self._find_data_links(soup, start_year, end_year)

        if data_links:
            # Step 3a: Follow each data link and parse
            for link_url, link_label in data_links:
                log.info(f"[{self.config['county']}] Following: {link_label} → {link_url}")
                self._scrape_page(link_url)
        else:
            # Step 3b: Parse the main page directly
            log.info(f"[{self.config['county']}] No sub-links found, parsing main page")
            self._parse_tables(soup, self.config["search_url"])

        # Step 4: Look for downloadable CSV/Excel/PDF links
        self._find_downloadable_files(soup)

        log.info(
            f"[{self.config['county']}] Scrape complete: "
            f"{len(self.results)} records, {len(self.errors)} errors"
        )
        return self.results

    def _find_data_links(self, soup: BeautifulSoup,
                         start_year: int, end_year: int) -> List[Tuple[str, str]]:
        """
        Find links on the page that point to surplus/overbid data.
        Looks for keywords: surplus, overbid, excess, foreclosure sale, completed.
        Filters by year range if years appear in the link text.
        """
        keywords = re.compile(
            r"(surplus|overbid|excess|completed.?sale|foreclosure.?sale|"
            r"unclaimed|overage|bid.?sheet)",
            re.IGNORECASE,
        )
        links = []
        for a_tag in soup.find_all("a", href=True):
            text = a_tag.get_text(strip=True)
            href = a_tag["href"]
            if not keywords.search(text) and not keywords.search(href):
                continue

            # Resolve relative URLs
            full_url = urljoin(self.config["search_url"], href)

            # Filter by year if a year appears in the text
            year_match = re.search(r"20\d{2}", text)
            if year_match:
                year = int(year_match.group())
                if year < start_year or year > end_year:
                    continue

            links.append((full_url, text))

        return links

    def _scrape_page(self, url: str):
        """Fetch a single page and extract table data."""
        resp = self.session.get(url)
        if not resp:
            self.errors.append(f"Failed to fetch: {url}")
            return
        soup = BeautifulSoup(resp.text, "lxml")
        self._parse_tables(soup, url)

        # Also check for PDF links on this page (bid sheets)
        self._find_downloadable_files(soup)

    def _parse_tables(self, soup: BeautifulSoup, source_url: str):
        """
        Parse HTML tables from the page. Identify which columns map to
        our required fields using fuzzy keyword matching.
        """
        tables = soup.find_all("table")
        if not tables:
            # Some county sites use div-based layouts instead of tables
            self._parse_div_layout(soup, source_url)
            return

        for table in tables:
            rows = table.find_all("tr")
            if len(rows) < 2:
                continue  # Need at least header + 1 data row

            # Find header row
            header_row = rows[0]
            headers = [th.get_text(strip=True).upper()
                       for th in header_row.find_all(["th", "td"])]

            # Map headers to our field names
            col_map = self._map_columns(headers)
            if not col_map:
                continue  # No recognizable columns in this table

            log.info(
                f"[{self.config['county']}] Found table with "
                f"{len(rows)-1} rows, mapped columns: {col_map}"
            )

            # Parse data rows
            for row in rows[1:]:
                cells = [td.get_text(strip=True)
                         for td in row.find_all(["td", "th"])]
                if len(cells) < len(headers):
                    continue

                record = self._row_to_asset(cells, col_map, source_url, row)
                if record:
                    self.results.append(record)

    def _map_columns(self, headers: List[str]) -> Dict[str, int]:
        """
        Map raw column headers to standard field names.
        Returns {field_name: column_index} for recognized columns.
        """
        COLUMN_PATTERNS = {
            "case_number": [
                "CASE", "CASE NUMBER", "CASE #", "CASE NO", "FILE NUMBER",
                "FORECLOSURE NUMBER", "FC#", "FC #", "FC_NUM", "RECEPTION",
            ],
            "property_address": [
                "PROPERTY ADDRESS", "ADDRESS", "LOCATION",
                "PROPERTY DESCRIPTION", "SITUS", "STREET",
            ],
            "mailing_address": [
                "MAILING ADDRESS", "MAILING", "MAIL ADDRESS",
                "OWNER ADDRESS", "BORROWER ADDRESS",
            ],
            "owner_of_record": [
                "OWNER", "GRANTOR", "BORROWER", "TAXPAYER", "DEFENDANT",
                "TITLE HOLDER", "ASSESSED TO", "OWNER NAME", "OWNER OF RECORD",
            ],
            "overbid_amount": [
                "WINNING BID", "WINNING_BID", "GRANTEE BID", "SALE PRICE",
                "SOLD AMOUNT", "BID AMOUNT", "PURCHASE PRICE", "AMOUNT BID",
                "BID", "PREMIUM",
            ],
            "total_indebtedness": [
                "TOTAL INDEBTEDNESS", "TOTAL_DEBT", "TOTAL DEBT",
                "JUDGMENT AMOUNT", "AMOUNT DUE", "OUTSTANDING BALANCE",
                "FACE AMOUNT", "LIEN COST", "BALANCE DUE", "DEBT",
            ],
            "estimated_surplus": [
                "SURPLUS", "EXCESS", "OVERBID", "SURPLUS AMOUNT",
                "SURPLUS_AMOUNT", "OVERAGE", "EXCESS PROCEEDS",
            ],
            "sale_date": [
                "SALE DATE", "SALE_DATE", "SOLD DATE", "AUCTION DATE",
                "DATE OF SALE", "DATE",
            ],
        }

        col_map = {}
        for field, patterns in COLUMN_PATTERNS.items():
            for i, header in enumerate(headers):
                for pattern in patterns:
                    if pattern in header or header in pattern:
                        col_map[field] = i
                        break
                if field in col_map:
                    break

        # Must have at least one identifying field to be useful
        has_identity = any(f in col_map for f in
                          ("case_number", "property_address", "owner_of_record"))
        has_financial = any(f in col_map for f in
                           ("overbid_amount", "estimated_surplus", "total_indebtedness"))

        if has_identity and has_financial:
            return col_map
        return {}

    def _row_to_asset(self, cells: List[str], col_map: Dict[str, int],
                      source_url: str,
                      row_element: Any = None) -> Optional[Dict]:
        """
        Convert a single table row into a pipeline-compatible asset dict.
        Runs the Zombie Algorithm for absentee detection.
        """
        def cell(field: str) -> Optional[str]:
            idx = col_map.get(field)
            if idx is not None and idx < len(cells):
                val = cells[idx].strip()
                return val if val else None
            return None

        # Extract raw values
        case_num = cell("case_number") or "UNKNOWN"
        prop_addr = cell("property_address")
        mail_addr = cell("mailing_address")
        owner = clean_owner(cell("owner_of_record"))
        bid = clean_money(cell("overbid_amount"))
        debt = clean_money(cell("total_indebtedness"))
        surplus = clean_money(cell("estimated_surplus"))
        sale_date = parse_date(cell("sale_date"))

        # Skip rows with no usable data
        if not prop_addr and not owner and not case_num:
            return None

        # If surplus isn't directly provided, compute it
        if surplus is None and bid is not None and debt is not None:
            surplus = round(bid - debt, 2)

        # Skip negative surplus (no money to recover)
        if surplus is not None and surplus <= 0:
            return None

        # --- THE ZOMBIE ALGORITHM ---
        is_absentee, absentee_reason = detect_absentee(prop_addr, mail_addr)

        # Check for PDF bid sheet link in the row
        pdf_link = None
        if row_element:
            for a in row_element.find_all("a", href=True):
                href = a["href"].lower()
                if href.endswith(".pdf") or "bid" in href or "sheet" in href:
                    pdf_link = urljoin(source_url, a["href"])
                    break

        # Generate recorder link
        recorder_link = None
        if owner and owner != "Unknown":
            recorder_link = self.config["recorder_url_template"].replace(
                "{owner}", quote_plus(owner)
            )

        # Build the asset dict — matches pipeline.ingest_asset() signature
        asset = {
            "county": self.config["county"],
            "state": self.config["state"],
            "asset_type": self.config["asset_type"],
            "case_number": case_num,
            "property_address": normalize_address(prop_addr) or prop_addr,
            "owner_of_record": owner,
            "estimated_surplus": surplus,
            "total_indebtedness": debt,
            "overbid_amount": bid,
            "sale_date": sale_date,
            "lien_type": "Deed of Trust",  # Default for CO foreclosures
            "recorder_link": recorder_link,
            "source_name": self.config.get("source_name", f"hunter_{self.config['county'].lower()}"),
            "source_file": f"hunter:{self.config['county'].lower()}:{source_url[:80]}",

            # Intelligence fields (not in pipeline schema but useful)
            "_mailing_address": mail_addr,
            "_is_absentee": is_absentee,
            "_absentee_reason": absentee_reason,
            "_pdf_link": pdf_link,
            "_scraped_at": datetime.utcnow().isoformat() + "Z",
        }

        return asset

    def _parse_div_layout(self, soup: BeautifulSoup, source_url: str):
        """
        Fallback parser for sites that use div/card layouts instead of tables.
        Looks for repeating div structures with keyword-labeled fields.
        """
        # Common patterns: dl/dt/dd pairs, labeled divs, card grids
        cards = soup.find_all("div", class_=re.compile(
            r"(card|result|record|item|listing)", re.IGNORECASE
        ))
        if not cards:
            return

        for card in cards:
            text = card.get_text(separator="\n", strip=True)
            # Try to extract key-value pairs from text
            asset = self._extract_from_text_block(text, source_url)
            if asset:
                self.results.append(asset)

    def _extract_from_text_block(self, text: str,
                                 source_url: str) -> Optional[Dict]:
        """
        Extract asset data from a freeform text block using regex patterns.
        Last resort when no clean table structure exists.
        """
        # Money pattern: $1,234.56 or 1234.56
        money_pattern = r"\$?[\d,]+\.?\d{0,2}"

        case_match = re.search(
            r"(?:case|file|fc)[#: ]*(\S+)", text, re.IGNORECASE
        )
        surplus_match = re.search(
            rf"(?:surplus|excess|overbid)[:\s]*({money_pattern})",
            text, re.IGNORECASE,
        )
        bid_match = re.search(
            rf"(?:bid|sale price|winning)[:\s]*({money_pattern})",
            text, re.IGNORECASE,
        )
        debt_match = re.search(
            rf"(?:debt|judgment|indebtedness|balance)[:\s]*({money_pattern})",
            text, re.IGNORECASE,
        )
        owner_match = re.search(
            r"(?:owner|borrower|grantor|defendant)[:\s]*([A-Z][A-Za-z\s,\.]+)",
            text, re.IGNORECASE,
        )
        date_match = re.search(
            r"(?:sale date|date of sale|sold)[:\s]*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})",
            text, re.IGNORECASE,
        )

        if not (surplus_match or bid_match):
            return None

        surplus = clean_money(surplus_match.group(1)) if surplus_match else None
        bid = clean_money(bid_match.group(1)) if bid_match else None
        debt = clean_money(debt_match.group(1)) if debt_match else None

        if surplus is None and bid and debt:
            surplus = round(bid - debt, 2)

        if surplus is not None and surplus <= 0:
            return None

        return {
            "county": self.config["county"],
            "state": self.config["state"],
            "asset_type": self.config["asset_type"],
            "case_number": case_match.group(1) if case_match else "UNKNOWN",
            "owner_of_record": clean_owner(
                owner_match.group(1) if owner_match else None
            ),
            "estimated_surplus": surplus,
            "total_indebtedness": debt,
            "overbid_amount": bid,
            "sale_date": parse_date(
                date_match.group(1) if date_match else None
            ),
            "lien_type": "Deed of Trust",
            "source_file": f"hunter:{self.config['county'].lower()}:text_extract",
            "_scraped_at": datetime.utcnow().isoformat() + "Z",
        }

    def _find_downloadable_files(self, soup: BeautifulSoup):
        """
        Look for downloadable CSV/Excel/PDF files linked on the page.
        These often contain the actual surplus data in cleaner format
        than the HTML tables.
        """
        file_links = []
        for a in soup.find_all("a", href=True):
            href = a["href"].lower()
            text = a.get_text(strip=True).lower()
            if any(ext in href for ext in (".csv", ".xlsx", ".xls", ".pdf")):
                if any(kw in text or kw in href for kw in
                       ("surplus", "overbid", "excess", "bid", "foreclosure")):
                    full_url = urljoin(self.config["search_url"], a["href"])
                    file_links.append((full_url, a.get_text(strip=True)))

        for url, label in file_links:
            log.info(f"[{self.config['county']}] Found downloadable file: {label} → {url}")
            # PDF files get handled by the OCR Patcher (Module C)
            # CSV/Excel files could be parsed inline but we log them for now
            if url.lower().endswith(".pdf"):
                # Store for OCR Patcher to process
                self.results.append({
                    "_type": "pdf_link",
                    "_url": url,
                    "_label": label,
                    "county": self.config["county"],
                })


# ============================================================================
# MODULE B2: REALAUCTION SCRAPER — Multi-County Platform
# ============================================================================

class RealForecloseScraper(ForensicScraper):
    """
    MODULE B2: RealForeclose.com Platform Parser
    =============================================
    Targets the ACTUAL auction data on {county}.realforeclose.com.

    VERIFIED LIVE (2026-02-07): Eagle, El Paso, Larimer, Mesa, Summit, Weld.

    Data Flow:
      1. Hit Calendar page → extract sale date links
      2. Hit Auction Preview page → get listing of all properties
      3. Parse rowA/rowB table rows (standard RealForeclose layout)
      4. Extract: case number, address, opening bid, winning bid, status
      5. Surplus = Winning Bid - Opening Bid (Opening Bid ≈ total indebtedness)
      6. Also follow detail links for per-property deep data

    URL Patterns (all relative to base_url):
      Calendar:   /index.cfm?zaction=USER&zmethod=CALENDAR
      Preview:    /index.cfm?zaction=AUCTION&zmethod=PREVIEW
      Background: /index.cfm?zaction=FORECLOSURE&zmethod=BACKGROUND
      Detail:     /index.cfm?zaction=AUCTION&zmethod=DETAILS&Aession=XXXXX

    CRITICAL: All internal link resolution uses urljoin(base_url, href).
    Never string-concatenate URLs.
    """

    # RealForeclose table row CSS classes
    ROW_CLASSES = re.compile(r"\brow[AB]\b", re.IGNORECASE)

    # Column header patterns specific to RealForeclose
    RF_COLUMN_MAP = {
        "case_number": re.compile(
            r"(case|file|foreclosure)\s*(#|no|number)?", re.IGNORECASE),
        "property_address": re.compile(
            r"(property|address|location|parcel)", re.IGNORECASE),
        "owner_of_record": re.compile(
            r"(owner|grantor|borrower|defendant)", re.IGNORECASE),
        "opening_bid": re.compile(
            r"(opening|start|minimum|upset)\s*(bid|price|amount)?", re.IGNORECASE),
        "winning_bid": re.compile(
            r"(winning|final|sold?|sale|high)\s*(bid|price|amount)?", re.IGNORECASE),
        "estimated_surplus": re.compile(
            r"(surplus|excess|over\s*bid|overage)", re.IGNORECASE),
        "sale_date": re.compile(
            r"(sale|auction|sold)\s*(date)?", re.IGNORECASE),
        "status": re.compile(
            r"(status|result|disposition)", re.IGNORECASE),
    }

    def scrape(self, start_year: int = 2020,
               end_year: int = 2026) -> List[Dict]:
        county = self.config["county"]
        base = self.config["base_url"]
        log.info(f"[{county}] RealForeclose scrape: {start_year}-{end_year}")

        # --- STEP 1: Hit Calendar to discover sale date links ---
        calendar_url = self.config.get("search_url") or \
            f"{base}/index.cfm?zaction=USER&zmethod=CALENDAR"
        resp = self.session.get(calendar_url)
        sale_date_links = []
        if resp:
            soup = BeautifulSoup(resp.text, "lxml")
            sale_date_links = self._extract_calendar_links(soup, base,
                                                           start_year, end_year)
            log.info(f"[{county}] Calendar: found {len(sale_date_links)} sale date links")
            # Also try parsing any tables already on the calendar page
            self._parse_rf_tables(soup, calendar_url)
        else:
            self.errors.append(f"Calendar page failed: {calendar_url}")

        # --- STEP 2: Hit Auction Preview (main listing) ---
        auction_url = self.config.get("auction_url") or \
            f"{base}/index.cfm?zaction=AUCTION&zmethod=PREVIEW"
        resp2 = self.session.get(auction_url)
        if resp2:
            soup2 = BeautifulSoup(resp2.text, "lxml")
            self._parse_rf_tables(soup2, auction_url)
            # Find links to individual auction detail pages
            detail_links = self._extract_detail_links(soup2, base)
            log.info(f"[{county}] Preview: found {len(detail_links)} detail links")
            for detail_url, label in detail_links[:50]:  # Cap at 50 to be respectful
                self._scrape_detail_page(detail_url)
        else:
            self.errors.append(f"Auction preview failed: {auction_url}")

        # --- STEP 3: Follow discovered sale date pages ---
        for sd_url, sd_label in sale_date_links[:24]:  # Cap at 24 months
            resp3 = self.session.get(sd_url)
            if resp3:
                soup3 = BeautifulSoup(resp3.text, "lxml")
                self._parse_rf_tables(soup3, sd_url)

        # --- STEP 4: Try Background page for extra context ---
        bg_url = f"{base}/index.cfm?zaction=FORECLOSURE&zmethod=BACKGROUND"
        resp4 = self.session.get(bg_url)
        if resp4:
            soup4 = BeautifulSoup(resp4.text, "lxml")
            self._parse_rf_tables(soup4, bg_url)
            self._find_downloadable_files(soup4)

        # --- STEP 5: Deduplicate ---
        seen = set()
        deduped = []
        for r in self.results:
            key = r.get("case_number") or r.get("property_address") or r.get("owner_of_record")
            if not key:
                deduped.append(r)
            elif key not in seen:
                seen.add(key)
                deduped.append(r)
        self.results = deduped

        log.info(
            f"[{county}] RealForeclose complete: "
            f"{len(self.results)} records, {len(self.errors)} errors"
        )
        return self.results

    def _extract_calendar_links(self, soup: BeautifulSoup, base_url: str,
                                start_year: int, end_year: int) -> List[Tuple[str, str]]:
        """Extract sale date links from the RealForeclose calendar page."""
        links = []
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            text = a_tag.get_text(strip=True)
            # RealForeclose calendar links contain zaction=AUCTION or date params
            if "zaction" in href.lower() or "auction" in href.lower() or \
               "session_date" in href.lower() or "ession_date" in href.lower():
                full_url = urljoin(base_url, href)
                # Filter by year if present
                year_match = re.search(r"20\d{2}", text + href)
                if year_match:
                    year = int(year_match.group())
                    if year < start_year or year > end_year:
                        continue
                links.append((full_url, text))
        return links

    def _extract_detail_links(self, soup: BeautifulSoup,
                              base_url: str) -> List[Tuple[str, str]]:
        """Extract links to individual auction detail pages."""
        links = []
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            text = a_tag.get_text(strip=True)
            if "details" in href.lower() or "zmethod=details" in href.lower():
                full_url = urljoin(base_url, href)
                links.append((full_url, text))
        return links

    def _scrape_detail_page(self, url: str):
        """Fetch an individual auction detail page and parse it."""
        resp = self.session.get(url)
        if not resp:
            return
        soup = BeautifulSoup(resp.text, "lxml")
        self._parse_rf_tables(soup, url)
        # Also try extracting from text blocks (detail pages may use div layout)
        self._parse_div_layout(soup, url)

    def _parse_rf_tables(self, soup: BeautifulSoup, source_url: str):
        """
        Parse RealForeclose-specific table structure.

        RealForeclose uses CSS classes 'rowA' and 'rowB' for alternating
        table rows. This parser specifically targets that pattern, then
        falls back to the generic table parser.
        """
        # --- PRIMARY: Look for rowA/rowB rows (RealForeclose signature) ---
        rf_rows = soup.find_all("tr", class_=self.ROW_CLASSES)
        if rf_rows:
            log.info(f"[{self.config['county']}] Found {len(rf_rows)} "
                     f"RealForeclose rowA/rowB entries")
            # Find the header row (usually the <tr> before the first rowA/rowB)
            header_row = None
            first_rf = rf_rows[0]
            prev = first_rf.find_previous_sibling("tr")
            if prev:
                header_cells = prev.find_all(["th", "td"])
                if header_cells:
                    headers = [c.get_text(strip=True) for c in header_cells]
                    header_row = headers

            col_map = self._map_rf_columns(header_row) if header_row else {}

            for row in rf_rows:
                cells = [td.get_text(strip=True) for td in row.find_all(["td", "th"])]
                if not cells:
                    continue
                record = self._rf_row_to_asset(cells, col_map, source_url, row)
                if record:
                    self.results.append(record)
            return

        # --- FALLBACK: Use generic table parser ---
        self._parse_tables(soup, source_url)

    def _map_rf_columns(self, headers: List[str]) -> Dict[str, int]:
        """Map RealForeclose header text to field names using regex matching."""
        col_map = {}
        for i, header in enumerate(headers):
            for field, pattern in self.RF_COLUMN_MAP.items():
                if pattern.search(header):
                    if field not in col_map:  # First match wins
                        col_map[field] = i
                    break
        return col_map

    def _rf_row_to_asset(self, cells: List[str], col_map: Dict[str, int],
                         source_url: str,
                         row_element: Any = None) -> Optional[Dict]:
        """
        Convert a RealForeclose rowA/rowB into an asset dict.

        SURPLUS CALCULATION:
          If 'estimated_surplus' column exists → use directly.
          Else: Surplus = Winning Bid - Opening Bid
          (Opening Bid ≈ total indebtedness on RealForeclose)
        """
        def cell(field: str) -> Optional[str]:
            idx = col_map.get(field)
            if idx is not None and idx < len(cells):
                val = cells[idx].strip()
                return val if val else None
            return None

        # If no column mapping, try positional (common RealForeclose layout):
        # [0]=Case, [1]=Address, [2]=OpeningBid, [3]=Status/WinningBid
        if not col_map and len(cells) >= 3:
            col_map = {"case_number": 0, "property_address": 1}
            # Detect which cell is money
            for i, c in enumerate(cells):
                if "$" in c or re.search(r"\d{1,3}(,\d{3})+", c):
                    if "opening_bid" not in col_map:
                        col_map["opening_bid"] = i
                    else:
                        col_map["winning_bid"] = i

        case_num = cell("case_number") or "UNKNOWN"
        prop_addr = cell("property_address")
        owner = clean_owner(cell("owner_of_record"))
        opening = clean_money(cell("opening_bid"))
        winning = clean_money(cell("winning_bid"))
        surplus = clean_money(cell("estimated_surplus"))
        sale_date = parse_date(cell("sale_date"))
        status = cell("status") or ""

        # Skip if no identifying data
        if not prop_addr and not owner and case_num == "UNKNOWN":
            return None

        # --- SURPLUS MATH ---
        # Opening Bid on RealForeclose = total indebtedness (debt owed)
        # Winning Bid = what the property actually sold for
        # Surplus = Winning - Opening (if positive, there's money for the owner)
        debt = opening
        bid = winning

        if surplus is None and bid is not None and debt is not None:
            computed = round(bid - debt, 2)
            if computed > 0:
                surplus = computed

        # Skip if no positive surplus and no financial data at all
        if surplus is not None and surplus <= 0:
            return None

        # Skip pending/cancelled unless they have surplus data
        status_upper = status.upper()
        if any(kw in status_upper for kw in ("CANCEL", "WITHDRAW", "POSTPONE")) \
                and surplus is None:
            return None

        # Generate recorder link
        recorder_link = None
        if owner and owner != "Unknown":
            recorder_link = self.config["recorder_url_template"].replace(
                "{owner}", quote_plus(owner)
            )

        # Check for PDF links in the row
        pdf_link = None
        if row_element:
            for a in row_element.find_all("a", href=True):
                href = a["href"].lower()
                if href.endswith(".pdf") or "bid" in href or "sheet" in href:
                    pdf_link = urljoin(source_url, a["href"])
                    break

        asset = {
            "county": self.config["county"],
            "state": self.config["state"],
            "asset_type": self.config["asset_type"],
            "case_number": case_num,
            "property_address": normalize_address(prop_addr) or prop_addr,
            "owner_of_record": owner,
            "estimated_surplus": surplus,
            "total_indebtedness": debt,
            "overbid_amount": bid,
            "sale_date": sale_date,
            "lien_type": "Deed of Trust",
            "recorder_link": recorder_link,
            "source_name": self.config.get("source_name",
                                           f"hunter_{self.config['county'].lower()}"),
            "source_file": f"realforeclose:{self.config['county'].lower()}:{source_url[:80]}",
            "_pdf_link": pdf_link,
            "_auction_status": status,
            "_scraped_at": datetime.utcnow().isoformat() + "Z",
        }
        return asset


# ============================================================================
# MODULE B3: GOVEASE SCRAPER — Mountain Resort Counties
# ============================================================================

class GTSSearchScraper(ForensicScraper):
    """
    MODULE B3: County-Hosted GTS Foreclosure Search Scraper
    =======================================================
    Targets ASP.NET foreclosure search applications hosted by counties.

    Used by: Adams, Arapahoe, Boulder, Douglas, Garfield

    Strategy:
      1. Hit the search page to get the form structure and any default results
      2. Look for ViewState/EventValidation (ASP.NET CSRF tokens)
      3. Submit search form with date range to get foreclosure listings
      4. Parse result tables (standard HTML tables, no rowA/rowB)
      5. Follow pagination if present
      6. Also check for downloadable reports (PDF/CSV links)
      7. If Garfield: also hit GovEase secondary endpoint

    IMPORTANT: These are ASP.NET apps — they require ViewState tokens.
    The StealthSession handles cookies automatically, which helps.
    """

    def scrape(self, start_year: int = 2020,
               end_year: int = 2026) -> List[Dict]:
        county = self.config["county"]
        log.info(f"[{county}] GTS search scrape: {start_year}-{end_year}")

        # --- STEP 1: Load search page ---
        search_url = self.config["search_url"]
        resp = self.session.get(search_url)
        if not resp:
            self.errors.append(f"Search page failed: {search_url}")
            return self.results

        soup = BeautifulSoup(resp.text, "lxml")

        # --- STEP 2: Parse any default results already on the page ---
        self._parse_tables(soup, search_url)

        # --- STEP 3: Try submitting search form with date range ---
        form = soup.find("form")
        if form:
            self._submit_gts_search(form, soup, search_url, start_year, end_year)

        # --- STEP 4: Look for data links (surplus, overbid, reports) ---
        data_links = self._find_data_links(soup, start_year, end_year)
        for link_url, link_label in data_links:
            log.info(f"[{county}] Following: {link_label} → {link_url}")
            self._scrape_page(link_url)

        # --- STEP 5: Check for downloadable files ---
        self._find_downloadable_files(soup)

        # --- STEP 6: Check reports URL if available ---
        reports_url = self.config.get("reports_url")
        if reports_url:
            resp2 = self.session.get(reports_url)
            if resp2:
                soup2 = BeautifulSoup(resp2.text, "lxml")
                self._parse_tables(soup2, reports_url)
                self._find_downloadable_files(soup2)

        # --- STEP 7: Garfield secondary — GovEase ---
        govease_url = self.config.get("govease_url")
        if govease_url:
            log.info(f"[{county}] Hitting GovEase secondary: {govease_url}")
            resp3 = self.session.get(govease_url)
            if resp3:
                soup3 = BeautifulSoup(resp3.text, "lxml")
                self._parse_tables(soup3, govease_url)
                self._find_downloadable_files(soup3)

        # --- STEP 8: Deduplicate ---
        seen = set()
        deduped = []
        for r in self.results:
            key = r.get("case_number") or r.get("property_address") or r.get("owner_of_record")
            if not key:
                deduped.append(r)
            elif key not in seen:
                seen.add(key)
                deduped.append(r)
        self.results = deduped

        log.info(
            f"[{county}] GTS search complete: "
            f"{len(self.results)} records, {len(self.errors)} errors"
        )
        return self.results

    def _submit_gts_search(self, form, soup: BeautifulSoup,
                           source_url: str,
                           start_year: int, end_year: int):
        """
        Submit the ASP.NET search form with date range parameters.

        GTS apps typically have:
          - __VIEWSTATE, __EVENTVALIDATION hidden fields
          - Date range inputs (start date, end date)
          - A "Search" submit button
        """
        # Extract ASP.NET hidden fields
        form_data = {}
        for inp in soup.find_all("input", {"type": "hidden"}):
            name = inp.get("name")
            value = inp.get("value", "")
            if name:
                form_data[name] = value

        # Find date inputs and set range
        date_inputs = soup.find_all("input", {"type": re.compile(r"text|date", re.IGNORECASE)})
        date_fields_set = 0
        for inp in date_inputs:
            name = inp.get("name", "").lower()
            inp_id = inp.get("id", "").lower()
            label = name + " " + inp_id
            if any(kw in label for kw in ("start", "from", "begin", "saledate1")):
                form_data[inp.get("name")] = f"01/01/{start_year}"
                date_fields_set += 1
            elif any(kw in label for kw in ("end", "to", "through", "saledate2")):
                form_data[inp.get("name")] = f"12/31/{end_year}"
                date_fields_set += 1

        if date_fields_set < 2:
            log.info(f"[{self.config['county']}] Could not identify date range fields")
            return

        # Find submit button
        submit = soup.find("input", {"type": "submit"})
        if submit and submit.get("name"):
            form_data[submit["name"]] = submit.get("value", "Search")

        # Determine form action URL
        action = form.get("action", "")
        post_url = urljoin(source_url, action) if action else source_url

        log.info(f"[{self.config['county']}] Submitting GTS search: "
                 f"{start_year}-{end_year} → {post_url}")

        resp = self.session.post(post_url, data=form_data)
        if resp:
            soup2 = BeautifulSoup(resp.text, "lxml")
            self._parse_tables(soup2, post_url)
            self._find_downloadable_files(soup2)

            # Follow pagination
            self._follow_pagination(soup2, post_url, form_data)

    def _follow_pagination(self, soup: BeautifulSoup, source_url: str,
                           form_data: dict, max_pages: int = 10):
        """Follow pagination links in GTS search results."""
        for page_num in range(2, max_pages + 1):
            # Look for "Next" or page number links
            next_link = None
            for a in soup.find_all("a", href=True):
                text = a.get_text(strip=True).lower()
                if text in ("next", "next >", ">>", str(page_num)):
                    href = a["href"]
                    if "javascript" in href.lower():
                        # ASP.NET postback — extract event target
                        match = re.search(r"__doPostBack\('([^']+)'", href)
                        if match:
                            page_data = form_data.copy()
                            page_data["__EVENTTARGET"] = match.group(1)
                            page_data["__EVENTARGUMENT"] = ""
                            resp = self.session.post(source_url, data=page_data)
                            if resp:
                                soup = BeautifulSoup(resp.text, "lxml")
                                new_count = len(self.results)
                                self._parse_tables(soup, source_url)
                                if len(self.results) == new_count:
                                    return  # No new records — stop
                                next_link = True
                    else:
                        full_url = urljoin(source_url, href)
                        resp = self.session.get(full_url)
                        if resp:
                            soup = BeautifulSoup(resp.text, "lxml")
                            new_count = len(self.results)
                            self._parse_tables(soup, full_url)
                            if len(self.results) == new_count:
                                return
                            next_link = True
                    break
            if not next_link:
                return


class CountyPageScraper(ForensicScraper):
    """
    MODULE B4: County .gov Page Scraper
    ====================================
    For counties that don't use vendor platforms: Pitkin, Routt,
    San Miguel, Grand.

    Strategy:
      1. Hit the county's foreclosure/surplus page
      2. Parse HTML tables and div layouts
      3. Follow links to surplus lists, overbid data, PDFs
      4. For Grand: download and parse the Foreclosure Book PDF
    """

    def scrape(self, start_year: int = 2020,
               end_year: int = 2026) -> List[Dict]:
        county = self.config["county"]
        log.info(f"[{county}] County page scrape: {start_year}-{end_year}")

        # Hit the main page
        search_url = self.config["search_url"]
        resp = self.session.get(search_url)
        if resp:
            soup = BeautifulSoup(resp.text, "lxml")
            self._parse_tables(soup, search_url)
            data_links = self._find_data_links(soup, start_year, end_year)
            for link_url, link_label in data_links:
                log.info(f"[{county}] Following: {link_label}")
                self._scrape_page(link_url)
            self._find_downloadable_files(soup)
        else:
            self.errors.append(f"County page failed: {search_url}")

        # Also try base_url if different from search_url
        base_url = self.config.get("base_url")
        if base_url and base_url != search_url:
            resp2 = self.session.get(base_url)
            if resp2:
                soup2 = BeautifulSoup(resp2.text, "lxml")
                self._parse_tables(soup2, base_url)
                self._find_downloadable_files(soup2)

        # Special: Grand County PDF Foreclosure Book
        pdf_url = self.config.get("pdf_url")
        if pdf_url:
            log.info(f"[{county}] Downloading PDF Foreclosure Book")
            pdf_bytes = self.session.download_pdf(pdf_url)
            if pdf_bytes and pdfplumber:
                self._parse_grand_pdf(pdf_bytes, pdf_url)
            elif pdf_bytes:
                self.results.append({
                    "_type": "pdf_link",
                    "_url": pdf_url,
                    "_label": "Foreclosure Book PDF",
                    "county": county,
                })

        # Deduplicate
        seen = set()
        deduped = []
        for r in self.results:
            key = r.get("case_number") or r.get("property_address") or r.get("owner_of_record")
            if not key:
                deduped.append(r)
            elif key not in seen:
                seen.add(key)
                deduped.append(r)
        self.results = deduped

        log.info(
            f"[{county}] County page complete: "
            f"{len(self.results)} records, {len(self.errors)} errors"
        )
        return self.results

    def _parse_grand_pdf(self, pdf_bytes: bytes, source_url: str):
        """Parse Grand County's Foreclosure Book PDF."""
        try:
            with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                for page in pdf.pages:
                    # Try table extraction first (most reliable)
                    tables = page.extract_tables()
                    for table in tables:
                        if not table or len(table) < 2:
                            continue
                        headers = [str(c).strip().upper() if c else ""
                                   for c in table[0]]
                        col_map = self._map_columns(headers)
                        if not col_map:
                            continue
                        for row in table[1:]:
                            cells = [str(c).strip() if c else "" for c in row]
                            record = self._row_to_asset(cells, col_map,
                                                        source_url)
                            if record:
                                self.results.append(record)

                    # Fallback: text extraction
                    text = page.extract_text()
                    if text:
                        asset = self._extract_from_text_block(text, source_url)
                        if asset:
                            self.results.append(asset)
        except Exception as e:
            log.error(f"[Grand] PDF parse error: {e}")
            self.errors.append(f"Grand PDF parse: {e}")


# ============================================================================
# MODULE C: OCR PATCHER — PDF Bid Sheet Extraction
# ============================================================================

class OCRPatcher:
    """
    Extracts surplus/overbid data from PDF bid sheets.

    Strategy:
      1. Download the PDF via StealthSession
      2. Open with pdfplumber (text extraction, not OCR)
      3. Search for "Overbid:", "Surplus:", "Excess:" patterns
      4. Extract dollar amounts adjacent to those keywords
      5. Patch the original asset record with found values

    pdfplumber handles text-based PDFs. For scanned image PDFs,
    you'd need Tesseract OCR — but county bid sheets are almost
    always text-based since they're generated digitally.
    """

    # Patterns to search for in PDF text
    SURPLUS_PATTERNS = [
        re.compile(r"(?:overbid|surplus|excess)[:\s]*\$?([\d,]+\.?\d{0,2})", re.IGNORECASE),
        re.compile(r"(?:excess proceeds|surplus funds)[:\s]*\$?([\d,]+\.?\d{0,2})", re.IGNORECASE),
    ]

    BID_PATTERNS = [
        re.compile(r"(?:winning bid|sale price|purchase price|bid amount)[:\s]*\$?([\d,]+\.?\d{0,2})", re.IGNORECASE),
    ]

    DEBT_PATTERNS = [
        re.compile(r"(?:total (?:indebtedness|debt|judgment)|judgment amount)[:\s]*\$?([\d,]+\.?\d{0,2})", re.IGNORECASE),
    ]

    OWNER_PATTERNS = [
        re.compile(r"(?:owner|grantor|borrower|defendant)[:\s]*([A-Z][A-Za-z\s,\.]+?)(?:\n|$)", re.IGNORECASE),
    ]

    CASE_PATTERNS = [
        re.compile(r"(?:case|file|reception)[# :]*(\d{4}[A-Z]*\d+|\d+-\w+-\d+)", re.IGNORECASE),
    ]

    def __init__(self, session: StealthSession):
        self.session = session
        if pdfplumber is None:
            log.warning(
                "pdfplumber not installed. OCR Patcher disabled. "
                "Install with: pip install pdfplumber"
            )

    def extract_from_pdf(self, pdf_url: str) -> Optional[Dict]:
        """
        Download and extract data from a single PDF bid sheet.
        Returns a partial asset dict with whatever fields were found.
        """
        if pdfplumber is None:
            return None

        log.info(f"[OCR] Downloading PDF: {pdf_url}")
        pdf_bytes = self.session.download_pdf(pdf_url)
        if not pdf_bytes:
            return None

        return self._parse_pdf_bytes(pdf_bytes, pdf_url)

    def _parse_pdf_bytes(self, pdf_bytes: bytes,
                         source_url: str = "") -> Optional[Dict]:
        """Extract data from raw PDF bytes."""
        if pdfplumber is None:
            return None

        try:
            with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                full_text = ""
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        full_text += page_text + "\n"

                    # Also try extracting tables from the PDF
                    tables = page.extract_tables()
                    for table in tables:
                        for row in table:
                            if row:
                                full_text += " | ".join(
                                    str(c) for c in row if c
                                ) + "\n"

        except Exception as e:
            log.error(f"[OCR] Failed to parse PDF: {e}")
            return None

        if not full_text.strip():
            log.warning(f"[OCR] PDF has no extractable text (may be scanned image)")
            return None

        # Extract fields using regex patterns
        result = {"_source_pdf": source_url}

        for pattern in self.SURPLUS_PATTERNS:
            match = pattern.search(full_text)
            if match:
                result["estimated_surplus"] = clean_money(match.group(1))
                break

        for pattern in self.BID_PATTERNS:
            match = pattern.search(full_text)
            if match:
                result["overbid_amount"] = clean_money(match.group(1))
                break

        for pattern in self.DEBT_PATTERNS:
            match = pattern.search(full_text)
            if match:
                result["total_indebtedness"] = clean_money(match.group(1))
                break

        for pattern in self.OWNER_PATTERNS:
            match = pattern.search(full_text)
            if match:
                result["owner_of_record"] = clean_owner(match.group(1))
                break

        for pattern in self.CASE_PATTERNS:
            match = pattern.search(full_text)
            if match:
                result["case_number"] = match.group(1)
                break

        # Compute surplus if we got bid and debt but not surplus directly
        if ("estimated_surplus" not in result
                and result.get("overbid_amount") and result.get("total_indebtedness")):
            surplus = result["overbid_amount"] - result["total_indebtedness"]
            if surplus > 0:
                result["estimated_surplus"] = round(surplus, 2)

        return result if len(result) > 1 else None  # >1 because _source_pdf always present

    def patch_records(self, records: List[Dict]) -> List[Dict]:
        """
        For any record missing surplus but having a PDF link,
        download the PDF and patch the record.
        """
        patched_count = 0
        for record in records:
            # Skip non-asset entries (like pdf_link placeholders)
            if record.get("_type") == "pdf_link":
                continue

            pdf_url = record.get("_pdf_link")
            if not pdf_url:
                continue

            # Only patch if surplus is missing
            if record.get("estimated_surplus"):
                continue

            log.info(f"[OCR] Patching record {record.get('case_number', '?')} from PDF")
            pdf_data = self.extract_from_pdf(pdf_url)
            if pdf_data:
                # Merge PDF data into record (don't overwrite existing values)
                for key, val in pdf_data.items():
                    if key.startswith("_"):
                        record[key] = val
                    elif not record.get(key):
                        record[key] = val
                        log.info(f"  → Patched {key} = {val}")

                patched_count += 1

        log.info(f"[OCR] Patched {patched_count} records from PDF bid sheets")
        return records


# ============================================================================
# MODULE D: LIEN WIPER — Surplus Math & Whale Classification
# ============================================================================

class LienWiper:
    """
    Financial analysis module:
      1. Computes Surplus_Liquidity = Bid - Total_Debt
      2. Detects WHALE records ($100K+ surplus, no junior liens)
      3. Calculates fee estimates based on statute tier
      4. Flags records by litigation quality

    Classification tiers:
      WHALE    — Surplus > $100K, no junior liens detected
      PRIME    — Surplus > $25K
      VIABLE   — Surplus > $5K
      MARGINAL — Surplus > $1K
      SKIP     — Surplus ≤ $1K (not worth attorney time)
    """

    THRESHOLDS = {
        "WHALE": 100_000,
        "PRIME": 25_000,
        "VIABLE": 5_000,
        "MARGINAL": 1_000,
    }

    # Keywords that indicate junior liens (risk factors)
    LIEN_VULTURE_KEYWORDS = [
        "HOA", "HOMEOWNERS ASSOCIATION", "BANK OF AMERICA", "WELLS FARGO",
        "CHASE", "CITIBANK", "US BANK", "NATIONSTAR", "OCWEN",
        "IRS", "INTERNAL REVENUE", "STATE TAX", "DEPARTMENT OF REVENUE",
        "SECOND DEED", "JUNIOR LIEN", "MECHANIC", "JUDGMENT CREDITOR",
    ]

    def classify(self, records: List[Dict]) -> List[Dict]:
        """
        Run financial analysis on all records.
        Adds classification fields to each record dict.
        """
        for record in records:
            if record.get("_type") == "pdf_link":
                continue

            surplus = record.get("estimated_surplus")
            bid = record.get("overbid_amount")
            debt = record.get("total_indebtedness")

            # Recompute surplus if needed
            if surplus is None and bid is not None and debt is not None:
                surplus = round(bid - debt, 2)
                record["estimated_surplus"] = surplus

            if surplus is None or surplus <= 0:
                record["_classification"] = "SKIP"
                record["_skip_reason"] = "no_positive_surplus"
                continue

            # --- CLASSIFICATION ---
            classification = "SKIP"
            for tier, threshold in self.THRESHOLDS.items():
                if surplus >= threshold:
                    classification = tier
                    break

            # --- JUNIOR LIEN DETECTION ---
            # Check all text fields for vulture keywords
            junior_lien_count = 0
            lien_flags = []
            text_to_check = " ".join(
                str(v) for v in record.values() if isinstance(v, str)
            ).upper()

            for keyword in self.LIEN_VULTURE_KEYWORDS:
                if keyword in text_to_check:
                    junior_lien_count += 1
                    lien_flags.append(keyword)

            # WHALE requires $100K+ AND zero junior liens
            if classification == "WHALE" and junior_lien_count > 0:
                classification = "PRIME"  # Downgrade — liens detected

            # --- FEE ESTIMATE ---
            # Based on Colorado statute tiers
            sale_date = record.get("sale_date")
            fee_pct = 0.33  # Default: attorney exclusive window
            if sale_date:
                try:
                    sale_dt = datetime.strptime(sale_date, "%Y-%m-%d")
                    days_since = (datetime.utcnow() - sale_dt).days
                    if days_since <= 180:
                        fee_pct = 0.33  # Unregulated
                    elif days_since <= 730:
                        fee_pct = 0.20  # Finder eligible, 20% cap
                    else:
                        fee_pct = 0.10  # Treasury, 10% cap
                    record["_days_since_sale"] = days_since
                except ValueError:
                    pass

            estimated_fee = round(surplus * fee_pct, 2)

            # --- WRITE RESULTS ---
            record["_classification"] = classification
            record["_junior_lien_count"] = junior_lien_count
            record["_junior_lien_flags"] = lien_flags
            record["_fee_pct"] = fee_pct
            record["_estimated_fee"] = estimated_fee
            record["_litigation_quality"] = self._assess_quality(record)

        return records

    def _assess_quality(self, record: Dict) -> str:
        """
        Overall litigation quality score based on data completeness.
        A = All fields present, high surplus, no liens, absentee owner
        B = Most fields, decent surplus
        C = Minimum viable — needs manual enrichment
        D = Missing critical fields — probably not actionable
        """
        score = 0

        # Has surplus > 0
        if record.get("estimated_surplus", 0) > 0:
            score += 1

        # Has owner name
        if record.get("owner_of_record") and record["owner_of_record"] != "Unknown":
            score += 1

        # Has sale date
        if record.get("sale_date"):
            score += 1

        # Has property address
        if record.get("property_address"):
            score += 1

        # Has case number
        if record.get("case_number") and record["case_number"] != "UNKNOWN":
            score += 1

        # No junior liens
        if record.get("_junior_lien_count", 0) == 0:
            score += 1

        # Absentee owner (bonus — easier to contact)
        if record.get("_is_absentee"):
            score += 1

        # High surplus
        if record.get("estimated_surplus", 0) >= 25_000:
            score += 1

        if score >= 7:
            return "A"
        elif score >= 5:
            return "B"
        elif score >= 3:
            return "C"
        else:
            return "D"

    def summary(self, records: List[Dict]) -> dict:
        """Generate classification summary."""
        counts = {"WHALE": 0, "PRIME": 0, "VIABLE": 0, "MARGINAL": 0, "SKIP": 0}
        total_surplus = 0
        total_fees = 0

        for r in records:
            cls = r.get("_classification", "SKIP")
            counts[cls] = counts.get(cls, 0) + 1
            if cls != "SKIP":
                total_surplus += r.get("estimated_surplus", 0)
                total_fees += r.get("_estimated_fee", 0)

        return {
            "classification_counts": counts,
            "total_surplus_found": round(total_surplus, 2),
            "total_estimated_fees": round(total_fees, 2),
            "actionable_records": sum(
                v for k, v in counts.items() if k != "SKIP"
            ),
        }


# ============================================================================
# ORCHESTRATOR — Ties all 4 modules together
# ============================================================================

def _build_config_map() -> Dict[str, dict]:
    """Build the complete county config map across all platforms."""
    configs = {
        "Denver": DENVER_CONFIG,
        "Jefferson": JEFFERSON_CONFIG,
    }
    configs.update(REALFORECLOSE_COUNTIES)
    configs.update(GTS_COUNTIES)
    configs.update(COUNTY_PAGE_COUNTIES)
    return configs


# All Colorado counties with active scraper support
ALL_COLORADO_COUNTIES = list(_build_config_map().keys())


def _get_scraper_class(config: dict):
    """Return the correct scraper class based on platform.

    Platform routing:
      realforeclose → RealForecloseScraper (rowA/rowB calendar parser)
      gts          → GTSSearchScraper (ASP.NET form search)
      county_page  → CountyPageScraper (.gov page + PDF support)
      standard     → ForensicScraper (generic HTML table parser)
    """
    platform = config.get("platform", "standard")
    if platform == "realforeclose":
        return RealForecloseScraper
    elif platform == "gts":
        return GTSSearchScraper
    elif platform == "county_page":
        return CountyPageScraper
    return ForensicScraper


def _update_scraper_registry(source_name: str, records_count: int, status: str):
    """Update scraper_registry with run results after each county scrape."""
    try:
        import sqlite3
        from pathlib import Path
        db_path = Path(__file__).resolve().parent.parent / "data" / "verifuse.db"
        if not db_path.exists():
            return
        conn = sqlite3.connect(str(db_path))
        conn.execute("""
            UPDATE scraper_registry SET
                last_run_at = datetime('now'),
                last_run_status = ?,
                records_produced = records_produced + ?
            WHERE scraper_name = ?
        """, (status, records_count, source_name))
        conn.commit()
        conn.close()
    except Exception as e:
        log.debug(f"Registry update failed for {source_name}: {e}")


def run_hunter(
    counties: Optional[List[str]] = None,
    start_year: int = 2020,
    end_year: int = 2026,
    output_csv: Optional[str] = None,
) -> Dict:
    """
    Main entry point for the VeriFuse Hunter Engine.

    Args:
        counties:   List of county names to scrape.
                    Default: ALL_COLORADO_COUNTIES (Total Colorado Domination).
                    Pass ["Denver", "Jefferson"] for original behavior.
                    Pass "ALL" or None for all counties.
        start_year: Earliest year to search. Default: 2020
        end_year:   Latest year to search. Default: 2026
        output_csv: Optional path to save results as CSV.

    Returns:
        Dict with keys: records, summary, session_stats, errors
    """
    CONFIGS = _build_config_map()

    if counties is None or counties == ["ALL"] or counties == "ALL":
        counties = ALL_COLORADO_COUNTIES

    # --- INITIALIZE MODULES ---
    session = StealthSession()
    ocr = OCRPatcher(session)
    lien_wiper = LienWiper()
    all_records = []
    all_errors = []

    print("=" * 70)
    print("VERIFUSE HUNTER ENGINE — COLORADO DRAGNET")
    print(f"Target Counties: {', '.join(counties)} ({len(counties)} total)")
    print(f"Date Range: {start_year} - {end_year}")
    print("=" * 70)

    # --- SCRAPE EACH COUNTY ---
    for county in counties:
        config = CONFIGS.get(county)
        if not config:
            log.warning(f"No config for county: {county}. Skipping.")
            all_errors.append(f"No config for county: {county}")
            continue

        platform = config.get("platform", "standard")
        print(f"\n>> Scraping {county} County ({platform})...")
        ScraperClass = _get_scraper_class(config)
        scraper = ScraperClass(session, config)

        try:
            records = scraper.scrape(start_year, end_year)
            all_records.extend(records)
            all_errors.extend(scraper.errors)

            # Update scraper registry with run results
            _update_scraper_registry(
                config.get("source_name", ""),
                len(records),
                "SUCCESS" if not scraper.errors else "PARTIAL",
            )

        except requests.exceptions.HTTPError as e:
            status = e.response.status_code if e.response is not None else 0
            _update_scraper_registry(config.get("source_name", ""), 0, f"FAILED:{status}")
            if status == 404:
                msg = f"[RECOVERY AGENT] 404 on {county}. Target URL moved or removed."
                log.error(msg)
                print(f"  {msg}")
                print(f"  [ACTION] Flagging {county} for URL audit. Skipping to protect pipeline.")
                all_errors.append(f"{county}: 404 — URL requires audit")
                continue
            elif status in (403, 429):
                msg = f"[CRITICAL] WAF BLOCK on {county} (HTTP {status}). Halting dragnet to protect IP."
                log.critical(msg)
                print(f"  {msg}")
                all_errors.append(f"{county}: {status} — IP may be flagged")
                break
            else:
                msg = f"[RECOVERY AGENT] HTTP {status} on {county}. Skipping."
                log.error(msg)
                print(f"  {msg}")
                all_errors.append(f"{county}: HTTP {status}")
                continue

        except (requests.exceptions.ConnectionError,
                requests.exceptions.ConnectTimeout,
                requests.exceptions.ReadTimeout) as e:
            _update_scraper_registry(config.get("source_name", ""), 0, "FAILED:CONNECTION")
            msg = f"[RECOVERY AGENT] Connection Dead on {county}. Skipping."
            log.error(msg)
            print(f"  {msg}")
            all_errors.append(f"{county}: connection_dead — {type(e).__name__}")
            continue

        except Exception as e:
            _update_scraper_registry(config.get("source_name", ""), 0, "FAILED")
            msg = f"[RECOVERY AGENT] Unhandled error on {county}: {e}"
            log.error(msg)
            print(f"  {msg}")
            all_errors.append(f"{county}: {e}")
            continue

    # --- SEPARATE ACTUAL RECORDS FROM PDF LINK PLACEHOLDERS ---
    pdf_links = [r for r in all_records if r.get("_type") == "pdf_link"]
    asset_records = [r for r in all_records if r.get("_type") != "pdf_link"]

    # --- PROCESS PDF LINKS ---
    for pdf_entry in pdf_links:
        log.info(f"[OCR] Processing discovered PDF: {pdf_entry.get('_url')}")
        pdf_data = ocr.extract_from_pdf(pdf_entry["_url"])
        if pdf_data:
            pdf_data["county"] = pdf_entry["county"]
            pdf_data["state"] = "CO"
            pdf_data["asset_type"] = "FORECLOSURE_SURPLUS"
            pdf_data["source_file"] = f"hunter:pdf:{pdf_entry['_url'][:60]}"
            asset_records.append(pdf_data)

    # --- OCR PATCH (fill missing surplus from linked PDFs) ---
    print(f"\n▶ OCR Patching {len(asset_records)} records...")
    asset_records = ocr.patch_records(asset_records)

    # --- LIEN WIPER (classify all records) ---
    print(f"\n▶ Running Lien Wiper analysis...")
    asset_records = lien_wiper.classify(asset_records)

    # --- FILTER OUT SKIPS FOR FINAL OUTPUT ---
    actionable = [r for r in asset_records if r.get("_classification") != "SKIP"]

    # --- SUMMARY ---
    summary = lien_wiper.summary(asset_records)
    session_stats = session.stats()

    print("\n" + "=" * 70)
    print("HUNTER REPORT")
    print("=" * 70)
    print(f"Total records scraped:     {len(asset_records)}")
    print(f"Actionable (non-SKIP):     {summary['actionable_records']}")
    print(f"Total surplus found:       ${summary['total_surplus_found']:,.2f}")
    print(f"Estimated attorney fees:   ${summary['total_estimated_fees']:,.2f}")
    print(f"\nClassification breakdown:")
    for cls, count in summary["classification_counts"].items():
        marker = "**" if cls == "WHALE" else "  "
        print(f"  {marker} {cls:10s}: {count}")

    absentee_count = sum(1 for r in actionable if r.get("_is_absentee"))
    print(f"\nAbsentee owners detected:  {absentee_count}")
    print(f"Session requests made:     {session_stats['requests']}")
    print(f"Session errors:            {session_stats['errors']}")

    if all_errors:
        print(f"\nErrors ({len(all_errors)}):")
        for err in all_errors[:10]:
            print(f"  !! {err}")

    # --- OPTIONAL CSV EXPORT ---
    if output_csv and actionable:
        _export_csv(actionable, output_csv)

    return {
        "records": actionable,
        "all_records": asset_records,
        "summary": summary,
        "session_stats": session_stats,
        "errors": all_errors,
    }


def _export_csv(records: List[Dict], filepath: str):
    """Export actionable records to CSV for manual review or Airtable import."""
    import csv

    # Define column order (pipeline fields first, then intelligence fields)
    columns = [
        "county", "case_number", "property_address", "owner_of_record",
        "estimated_surplus", "overbid_amount", "total_indebtedness",
        "sale_date", "lien_type", "recorder_link",
        "_classification", "_litigation_quality", "_estimated_fee",
        "_fee_pct", "_days_since_sale",
        "_is_absentee", "_absentee_reason", "_mailing_address",
        "_junior_lien_count", "_junior_lien_flags",
        "_pdf_link", "_scraped_at",
    ]

    with open(filepath, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=columns, extrasaction="ignore")
        writer.writeheader()
        for record in records:
            writer.writerow(record)

    log.info(f"Exported {len(records)} records to {filepath}")
    print(f"\n✓ CSV exported: {filepath}")


# ============================================================================
# PIPELINE BRIDGE — Feed results into canonical verifuse pipeline
# ============================================================================

def ingest_to_pipeline(records: List[Dict], db_path: Optional[str] = None):
    """
    Feed hunter results into the canonical VeriFuse pipeline.

    This bridges Module B/C/D output → verifuse.core.pipeline.ingest_asset().
    Only call this when you want to persist results to the canonical DB.

    Args:
        records: List of asset dicts from run_hunter()
        db_path: Path to verifuse DB. Uses default if None.
    """
    import sqlite3
    from ..core.schema import DB_PATH, init_db
    from ..core.pipeline import ingest_asset, evaluate_all

    path = db_path or str(DB_PATH)
    conn = sqlite3.connect(path)
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA foreign_keys=ON")

    ingested = 0
    skipped = 0
    errors = 0

    seen_ids = set()
    for record in records:
        # Strip intelligence fields (prefixed with _) before ingestion
        clean = {k: v for k, v in record.items() if not k.startswith("_")}
        # Extract source_name for pipeline registry lookup (must match scraper_registry)
        source = clean.pop("source_name", clean.pop("source_file", "hunter_engine"))
        try:
            asset_id = ingest_asset(conn, clean, source)
            if asset_id in seen_ids:
                skipped += 1
            else:
                seen_ids.add(asset_id)
                ingested += 1
        except Exception as e:
            log.error(f"Ingestion error for {record.get('case_number')}: {e}")
            errors += 1

    # Run evaluation to promote through pipeline
    eval_results = evaluate_all(conn)
    conn.close()

    print(f"\n{'='*70}")
    print("PIPELINE INGESTION COMPLETE")
    print(f"{'='*70}")
    print(f"Ingested:  {ingested}")
    print(f"Skipped:   {skipped} (duplicates)")
    print(f"Errors:    {errors}")
    print(f"Evaluation: {eval_results}")

    return {"ingested": ingested, "skipped": skipped, "errors": errors,
            "evaluation": eval_results}


# ============================================================================
# COLAB / CLI ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    import sys
    # CLI: python hunter_engine.py [county1,county2,...] [start_year] [end_year]
    if len(sys.argv) > 1 and sys.argv[1].upper() != "ALL":
        target_counties = [c.strip() for c in sys.argv[1].split(",")]
    else:
        target_counties = ALL_COLORADO_COUNTIES

    try:
        start = int(sys.argv[2]) if len(sys.argv) > 2 else 2020
        end = int(sys.argv[3]) if len(sys.argv) > 3 else 2026
    except ValueError:
        print("Usage: hunter_engine.py [county1,county2,...|ALL] [start_year] [end_year]")
        sys.exit(1)

    results = run_hunter(
        counties=target_counties,
        start_year=start,
        end_year=end,
        output_csv="verifuse_hunter_results.csv",
    )
